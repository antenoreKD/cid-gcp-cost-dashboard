AWSTemplateFormatVersion: "2010-09-09"
Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups: 
      - 
        Label: 
          default: "To change"
        Parameters: 
          - Prefix
          - BucketName
          - GCPFullTableName
          - GCPPricingFullTableName
          - GCPFocusFullTableName
          - GCPConnectionName
          - Secret
          - SecretARN
          - KMSOwners
          - GlueCrontab
          - StartDate
      - 
        Label: 
          default: "Necessary config - DON'T CHANGE"
        Parameters: 
          - GCPBillingLocation
          - GCPPricingLocation
          - GCPFocusLocation
          - GCPJobBookmarkKeys
          - TargetCatalogDBName
    ParameterLabels: 
      Prefix:
        Type: String
        Description: Prefix for created resources
        Default: cidgcp

      BucketName:
        Type: String
        Description: Name for the S3 bucket used as the target

      GCPFullTableName:
        Type: CommaDelimitedList
        Description: GCP Billing Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\" # DOCUMENTATION

      GCPPricingFullTableName:
        Type: String
        Description: GCP Pricing Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\" # DOCUMENTATION

      GCPFocusFullTableName:
        Type: CommaDelimitedList
        Description: (Optional) GCP Focus Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\" # DOCUMENTATION

      GCPConnectionName:
        Type: String
        Description: GCP Connection Name
        Default: GoogleBigQueryConnection

      Secret:
        Type: String
        Description: Name in secrets manager of GCP Service Account JSON encoded in Base64 # ARN OR NAME

      SecretARN:
        Type: String
        Description: Name in secrets manager of GCP Service Account JSON encoded in Base64 # ARN OR NAME
      
      KMSOwners:
        Type: CommaDelimitedList
        Description: ARN of the KMS Owners. !!! Current User is mandatory !!!
      
      GlueCrontab:
        Type: String
        Description: "Time-based schedule for your jobs in AWS Glue. The definition of these schedules uses the Unix-like cron syntax. For more info check: https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html"
        Default: "0 4 * * ? *"
      
      StartDate:
        Type: String
        Description: 'Start date for data retrieval, leave empty to retrieve all. Format: YYYY-MM-DD HH:MM:SS'
        Default: "2024-01-01 00:00:00"

      GCPBillingLocation:
        Type: String
        Description: DON'T CHANGE - GCP Billing Location in S3
        Default: gcp_billing_export

      GCPPricingLocation:
        Type: String
        Description: DON'T CHANGE - GCP Pricing Location in S3
        Default: gcp_pricing_export
      
      GCPFocusLocation:
        Type: String
        Description: DON'T CHANGE - GCP Focus Location in S3
        Default: gcp_focus_export

      GCPJobBookmarkKeys:
        Type: String
        Description: DON'T CHANGE - GCP Job Bookmark Keys
        Default: export_time

      TargetCatalogDBName:
        Type: String
        Description: DON'T CHANGE - Target Catalog Database Name
        Default: gcpapp_db

Parameters:
  Prefix:
    Type: String
    Description: Prefix for created resources
    Default: cidgcp

  BucketName:
    Type: String
    Description: Name for the S3 bucket used as the target
  
  GCPFullTableName:
    Type: CommaDelimitedList
    AllowedPattern: "(?:[a-zA-Z0-9_-]+(?:\\.[a-zA-Z0-9_-]+)?\\.gcp_billing_export_v1_[a-zA-Z0-9_]+(?:\\.[a-zA-Z0-9_-]+)?)(?:,(?:[a-zA-Z0-9_-]+(?:\\.[a-zA-Z0-9_-]+)?\\.gcp_billing_export_v1_[a-zA-Z0-9_]+(?:\\.[a-zA-Z0-9_-]+)?))*"
    Description: GCP Billing Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\" # DOCUMENTATION

  GCPPricingFullTableName:
    Type: String
    AllowedPattern: "^[a-zA-Z0-9_-]+\\.[a-zA-Z0-9_-]+\\.[a-zA-Z0-9_]+$"
    Description: GCP Pricing Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\"

  GCPFocusFullTableName:
    Type: CommaDelimitedList
    Description: (Optional) GCP Focus Full Table Id List - To copy a table id, please click on the three dots on the right of the table name and select \"Copy ID\"
    Default: ""

  GCPConnectionName:
    Type: String
    Description: GCP Connection Name
    Default: GoogleBigQueryConnection
  
  Secret:
    Type: String
    Description: Name in secrets manager of GCP Service Account JSON encoded in Base64 # ARN OR NAME
  
  SecretARN:
    Type: String
    Description: Name in secrets manager of GCP Service Account JSON encoded in Base64 # ARN OR NAME
  
  KMSOwners:
    Type: CommaDelimitedList
    Description: ARN of the KMS Owners. !!! Current User is mandatory !!!
  
  GlueCrontab:
    Type: String
    Description: "Time-based schedule for your jobs in AWS Glue. The definition of these schedules uses the Unix-like cron syntax. For more info check: https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html"
    Default: "0 4 * * ? *"
  
  StartDate:
    Type: String
    Description: 'Start date for data retrieval, leave empty to retrieve all. Format: YYYY-MM-DD HH:MM:SS'
    AllowedPattern: "^(?:\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})?$"
    Default: "2024-01-01 00:00:00"

  GCPBillingLocation:
    Type: String
    Description: DON'T CHANGE - GCP Billing Location in S3
    Default: gcp_billing_export
  
  GCPPricingLocation:
    Type: String
    Description: DON'T CHANGE - GCP Pricing Location in S3
    Default: gcp_pricing_export
  
  GCPFocusLocation:
    Type: String
    Description: DON'T CHANGE - GCP Focus Location in S3
    Default: gcp_focus_export

  GCPJobBookmarkKeys:
    Type: String
    Description: DON'T CHANGE - GCP Job Bookmark Keys
    Default: export_time
  
  TargetCatalogDBName:
    Type: String
    Description: DON'T CHANGE - Target Catalog Database Name
    Default: gcpapp_db

Conditions:
  StartDateNotEmpty: !Not [!Equals [!Ref StartDate, '']]
  

Resources:
  GCPLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import boto3
          import cfnresponse
          
          glue_client = boto3.client('glue')
          
          script_location = os.environ.get('ScriptLocation')
          focus_script_location = os.environ.get('FocusScriptLocation')
          gcp_connection_name = os.environ.get('GCPConnectionName')
          gcp_job_bookmark_keys = os.environ.get('GCPJobBookmarkKeys')
          target_catalog_database_name = os.environ.get('TargetCatalogDBName')
          dynamodb_admin_job_table_name = os.environ.get('DynamoDBAdminJobTable')
          gcp_billing_full_table_names = os.environ.get('GCPFullTableName')
          if gcp_billing_full_table_names is not None:
              gcp_billing_full_table_names = gcp_billing_full_table_names.split(";")
          else:
              gcp_billing_full_table_names = []
          gcp_pricing_full_table_name = os.environ.get('GCPPricingFullTableName')
          gcp_focus_full_table_names = os.environ.get('GCPFocusFullTableName')
          if gcp_focus_full_table_names is not None:
              gcp_focus_full_table_names = gcp_focus_full_table_names.split(";")
          else:
              gcp_focus_full_table_names = []
          glue_role = os.environ.get('GlueRoleArn')
          billing_crawler_name = os.environ.get('BillingCrawlerName')
          pricing_crawler_name = os.environ.get('PricingCrawlerName')
          focus_crawler_name = os.environ.get('FocusCrawlerName')
          billing_target_s3_path = os.environ.get('BillingTargetS3Path')
          pricing_target_s3_path = os.environ.get('PricingTargetS3Path')
          focus_target_s3_path = os.environ.get('FocusTargetS3Path')
          security_configuration = os.environ.get('SecurityConfiguration')
          glue_crontab = os.environ.get('GlueCrontab')
          glue_job_trigger = os.environ.get('GlueJobTrigger')
          glue_focus_job_trigger = os.environ.get('FOCUSGlueJobTrigger')
          glue_crawler_trigger = os.environ.get('GlueCrawlerTrigger')
          focus_glue_crawler_trigger = os.environ.get('FocusGlueCrawlerTrigger')
          glue_workflow = os.environ.get('GlueWorkflow')
          glue_focus_workflow = os.environ.get('GlueFocusWorkflow')
          script_bucket = os.environ.get('ScriptBucketName')
          shuffle_bucket = "spark.shuffle.glue.s3ShuffleBucket=s3://"+script_bucket+"/shuffle/"
          kms_key_arn = os.environ.get('KmsKeyArn')
          
          def create_glue_job_trigger(job_names):
              trigger_name = glue_job_trigger
              actions = []
              for job_name in job_names:
                action = {
                          'JobName': job_name,
                          'Arguments': {}
                }
                actions.append(action)
              # Check if the triggers already exist
              try:
                  glue_client.get_trigger(Name=trigger_name)
                  print(f'trigger {trigger_name} exists, updating..')
                  try:
                      glue_client.update_trigger(
                          Name=trigger_name,
                          TriggerUpdate={
                              'Name': trigger_name,
                              'Description': "Trigger for running GCP Glue Jobs every 24 hours",
                              'Actions': actions,
                              #'WorkflowName': glue_workflow,
                              'Schedule': "cron("+glue_crontab+")"
                          }
                      )
                  except Exception as exc:
                      print(f'ERROR Updating {trigger_name}: {exc}')
                  return
              except glue_client.exceptions.EntityNotFoundException:
                  print(f'Creating {trigger_name}')
                  try:
                      glue_client.create_trigger(
                          Name=trigger_name,
                          #Description="Trigger for running " + job_name + " every 24 hours",
                          Description="Trigger for running GCP Glue Job every 24 hours",
                          Type='SCHEDULED',
                          Actions=actions,
                          StartOnCreation=True,
                          WorkflowName=glue_workflow,
                          Schedule="cron("+glue_crontab+")"
                      )
                  except Exception as exc:
                      print(f'ERROR Creating {trigger_name}: {exc}')
          
          def create_focus_glue_job_trigger(job_names):
              trigger_name = glue_focus_job_trigger
              actions = []
              for job_name in job_names:
                action = {
                          'JobName': job_name,
                          'Arguments': {}
                }
                actions.append(action)
              # Check if the triggers already exist
              try:
                  glue_client.get_trigger(Name=trigger_name)
                  print(f'trigger {trigger_name} exists, updating..')
                  try:
                      glue_client.update_trigger(
                          Name=trigger_name,
                          TriggerUpdate={
                              'Name': trigger_name,
                              'Description': "Trigger for running GCP FOCUS Glue Jobs every 24 hours",
                              'Actions': actions,
                              #'WorkflowName': glue_workflow,
                              'Schedule': "cron("+glue_crontab+")"
                          }
                      )
                  except Exception as exc:
                      print(f'ERROR Updating {trigger_name}: {exc}')
                  return
              except glue_client.exceptions.EntityNotFoundException:
                  print(f'Creating {trigger_name}')
                  try:
                      glue_client.create_trigger(
                          Name=trigger_name,
                          #Description="Trigger for running " + job_name + " every 24 hours",
                          Description="Trigger for running GCP FOCUS Glue Job every 24 hours",
                          Type='SCHEDULED',
                          Actions=actions,
                          StartOnCreation=True,
                          WorkflowName=glue_focus_workflow,
                          Schedule="cron("+glue_crontab+")"
                      )
                  except Exception as exc:
                      print(f'ERROR Creating {trigger_name}: {exc}')
          
          def create_glue_crawler_trigger(job_names,focus_job_names,billing_crawler_name,pricing_crawler_name,focus_crawler_name):
              trigger_name = glue_crawler_trigger
              focus_trigger_name = focus_glue_crawler_trigger
              conditions = []
              focus_conditions = []
              for job_name in job_names:
                condition = {
                    'LogicalOperator': "EQUALS",
                    'JobName': job_name,
                    'State': "SUCCEEDED"
                }
                conditions.append(condition)
              for focus_job_name in focus_job_names:
                focus_condition = {
                    'LogicalOperator': "EQUALS",
                    'JobName': focus_job_name,
                    'State': "SUCCEEDED"
                }
                focus_conditions.append(focus_condition)
              try:
                  # Check if the triggers already exist
                  glue_client.get_trigger(Name=trigger_name)
                  print(f'trigger {trigger_name} exists, updating..')
                  try: 
                      glue_client.update_trigger(
                          Name=trigger_name,
                          TriggerUpdate={
                              'Name': trigger_name,
                              'Description': "Trigger for running " + billing_crawler_name + " and " + pricing_crawler_name + " conditionally",
                              'Predicate': {
                                'Logical':'AND',
                                'Conditions': conditions
                              },
                              'Actions': [{
                                  'CrawlerName': billing_crawler_name,
                                  'Arguments': {}
                                },
                                {
                                  'CrawlerName': pricing_crawler_name,
                                  'Arguments': {}
                                }
                              ]
                          }
                      )
                  except Exception as exc:
                    print(f'ERROR updating trigger {trigger_name}: {exc}')
                  try: 
                      glue_client.update_trigger(
                          Name=focus_trigger_name,
                          TriggerUpdate={
                              'Name': focus_trigger_name,
                              'Description': "Trigger for running " + focus_crawler_name + " conditionally",
                              'Predicate': {
                                'Logical':'AND',
                                'Conditions': focus_conditions
                              },
                              'Actions': [{
                                  'CrawlerName': focus_crawler_name,
                                  'Arguments': {}
                                }
                              ]
                          }
                      )
                  except Exception as exc:
                    print(f'ERROR updating FOCUS trigger {trigger_name}: {exc}')
                  return
              except glue_client.exceptions.EntityNotFoundException:
                  print(f'Creating {trigger_name}')
                  try:
                      glue_client.create_trigger(
                          Name=trigger_name,
                          Description="Trigger for running " + billing_crawler_name + " and " + pricing_crawler_name + " conditionally",
                          Type='CONDITIONAL',
                          WorkflowName=glue_workflow,
                          StartOnCreation=True,
                          Predicate={
                            'Logical':'AND',
                            'Conditions': conditions
                          },
                          Actions=[{
                              'CrawlerName': billing_crawler_name,
                              'Arguments': {}
                            },
                            {
                              'CrawlerName': pricing_crawler_name,
                              'Arguments': {}
                            }
                          ]
                      )
                  except Exception as exc:
                    print(f'ERROR creating trigger {trigger_name}: {exc}')
                  print(f'Creating {focus_trigger_name}')
                  try:
                      glue_client.create_trigger(
                          Name=focus_trigger_name,
                          Description="Trigger for running " + focus_crawler_name + " conditionally",
                          Type='CONDITIONAL',
                          WorkflowName=glue_focus_workflow,
                          StartOnCreation=True,
                          Predicate={
                            'Logical':'AND',
                            'Conditions': focus_conditions
                          },
                          Actions=[{
                              'CrawlerName': focus_crawler_name,
                              'Arguments': {}
                            }
                          ]
                      )
                  except Exception as exc:
                    print(f'ERROR creating trigger {focus_trigger_name}: {exc}')
          
          def create_glue_job(job_name, table_name, target_s3_path, script_location, bookmark_keys):
              table_project, table_dataset, table_id = table_name.split('.')
              # check if job exists
              try:
                  glue_client.get_job(JobName=job_name)
                  return f'Job {job_name} already exists.'
              except glue_client.exceptions.EntityNotFoundException:
                  response = glue_client.create_job(
                      Name=job_name,
                      Role=glue_role,
                      Command={
                          'Name': 'glueetl',
                          'ScriptLocation': script_location
                      },
                      Connections={'Connections': [gcp_connection_name]},
                      DefaultArguments={
                          '--job-bookmark-option': "job-bookmark-disable",
                          '--enable-auto-scaling': "true",
                          '--enable-continuous-cloudwatch-log': "true",
                          '--enable-observability-metrics': "true",
                          '--enable-metrics': "true",
                          '--gcp_parent_project': table_project,
                          '--gcp_full_table_name': table_name,
                          '--gcp_connection_name': gcp_connection_name,
                          '--gcp_job_bookmark_keys': bookmark_keys,
                          '--gcp_materialization_dataset': table_dataset,
                          '--target_s3_path': target_s3_path,
                          '--target_catalog_database_name': target_catalog_database_name,
                          '--target_catalog_table_name': target_s3_path.rstrip('/').split('/')[-1],
                          '--dynamodb_admin_job_table_name': dynamodb_admin_job_table_name,
                          '--write-shuffle-files-to-s3': "true",
                          '--conf': shuffle_bucket,
                          '--spark.shuffle.storage.s3.serverSideEncryption.algorithm': "SSE-KMS",
                          '--spark.shuffle.storage.s3.serverSideEncryption.kms.key': kms_key_arn

                      },
                      SecurityConfiguration=security_configuration,
                      WorkerType="G.1X",
                      NumberOfWorkers=2,
                      GlueVersion="4.0"
                  )
                  if response != job_name:
                    #glue_client.start_job_run(JobName=job_name)
                    return f'Job {job_name} created successfully.'
                  else:
                    return f'Error creating Job {job_name}'
          
          def delete_glue_job(job_name):
              try:
                  glue_client.delete_job(JobName=job_name)
                  return f'deleting {job_name}'
              except Exception as exc:
                  return f'Error on deleting {job_name} : {exc}'
          
          def delete_trigger(trigger_name):
              try:
                  glue_client.delete_trigger(Name=trigger_name)
                  return f'deleting {trigger_name}'
              except Exception as exc:
                  return f'Error on deleting {trigger_name} : {exc}'
          
          def on_create_or_update():
              """ create jobs """
              response = {}
              job_names=[]
              focus_job_names=[]
          
              #Create jobs for each billing table
              for table_name in gcp_billing_full_table_names:
                  job_name = f'GcpApp_{table_name}'
                  job_names.append(job_name)
                  print(f'Creating Glue Job {job_name} ..')
                  response[table_name] = create_glue_job(
                      job_name=job_name,
                      table_name=table_name,
                      target_s3_path=billing_target_s3_path,
                      script_location=script_location,
                      bookmark_keys=gcp_job_bookmark_keys
                  )
          
              #Create job for pricing
              job_name = f'GcpApp_{gcp_pricing_full_table_name}'
              job_names.append(job_name)
              print(f'Creating Glue Job {job_name} ..')
              response[gcp_pricing_full_table_name] = create_glue_job(
                  job_name=job_name,
                  table_name=gcp_pricing_full_table_name,
                  target_s3_path=pricing_target_s3_path,
                  script_location=script_location,
                  bookmark_keys=gcp_job_bookmark_keys
              )

              #Create jobs for each focus table
              for table_name in gcp_focus_full_table_names:
                  job_name = f'GcpApp_Focus_{table_name}'
                  focus_job_names.append(job_name)
                  print(f'Creating Focus Glue Job {job_name} ..')
                  response[table_name] = create_glue_job(
                      job_name=job_name,
                      table_name=table_name,
                      target_s3_path=focus_target_s3_path,
                      script_location=focus_script_location,
                      bookmark_keys="x_ExportTime"
                  )
          
              # Create triggers for jobs and crawlers
              print(f"Updating triggers for CID GCP Glue Jobs..")
              create_glue_job_trigger(job_names)
              create_focus_glue_job_trigger(focus_job_names)
              print(f"Updating triggers for {billing_crawler_name} and {pricing_crawler_name}..")
              create_glue_crawler_trigger(job_names,focus_job_names,billing_crawler_name,pricing_crawler_name,focus_crawler_name)
              print(f"Start run of the workflow: {glue_workflow}")
              glue_client.start_workflow_run(Name=glue_workflow)
              print(f"Start run of the workflow: {glue_focus_workflow}")
              glue_client.start_workflow_run(Name=glue_focus_workflow)
              return response
          
          def on_delete():
              response = {}
              job_names=[]
          
              #Delete jobs for each billing table
              for table_name in gcp_billing_full_table_names:
                  job_name = f'GcpApp_{table_name}'
                  job_names.append(job_name)
                  response[table_name] = delete_glue_job(
                      job_name=job_name,
                 )
          
              #Delete job for pricing
              job_name = f'GcpApp_{gcp_pricing_full_table_name}'
              job_names.append(job_name)
              response[gcp_pricing_full_table_name] = delete_glue_job(
                  job_name=job_name,
              )
              
              #Delete jobs for each focus table
              for table_name in gcp_focus_full_table_names:
                  job_name = f'GcpApp_Focus_{table_name}'
                  job_names.append(job_name)
                  response[table_name] = delete_glue_job(
                      job_name=job_name,
                  )

              #Delete triggers
              response[glue_job_trigger]=delete_trigger(glue_job_trigger)
              response[glue_crawler_trigger]=delete_trigger(glue_crawler_trigger)
              
              return response
          
          def lambda_handler(event, context):
              request_type = event.get('RequestType', 'Undef')
              region = boto3.session.Session().region_name
              log_url = f"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logEvent:group={context.log_group_name};stream={context.log_stream_name}"
              try:
                  if request_type in ( 'Create', 'Update') :
                      response = on_create_or_update()
                  elif request_type == 'Delete':
                      response = on_delete()
                  else: raise Exception('Unexpected request_type = {request_type}')
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response)
              except Exception as exc:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(exc) + ' See logs: ' + log_url})
      Handler: index.lambda_handler
      Role: !GetAtt GCPLambdaRole.Arn
      FunctionName: !Sub "${Prefix}-GCPGlueGenerator"
      Runtime: python3.12
      Timeout: 60
      ReservedConcurrentExecutions: 100
      KmsKeyArn: !GetAtt KMSKey.Arn
      Environment:
        Variables:
          GCPFullTableName: !Join [";", !Ref GCPFullTableName]
          GCPFocusFullTableName: !Join [";", !Ref GCPFocusFullTableName]
          ScriptBucketName: !Sub "${Prefix}-${BucketName}-script"
          ScriptLocation: !Sub "s3://${Prefix}-${BucketName}-script/glue_script.py"      
          FocusScriptLocation: !Sub "s3://${Prefix}-${BucketName}-script/focus_glue_script.py"   
          GCPConnectionName: !Ref GCPConnectionName
          GCPJobBookmarkKeys: !Ref GCPJobBookmarkKeys
          BillingTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPBillingLocation}/"
          PricingTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPPricingLocation}/"
          FocusTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPFocusLocation}/"
          TargetCatalogDBName: !Ref TargetCatalogDBName
          DynamoDBAdminJobTable: !Select [1, !Split ['/', !GetAtt DynamoDBAdminJobTable.Arn]]
          GCPPricingLocation: !Ref GCPPricingLocation
          GCPPricingFullTableName: !Ref GCPPricingFullTableName
          GlueRoleArn: !GetAtt GlueRole.Arn
          SecurityConfiguration: !Ref GlueSecurityConfiguration
          BillingCrawlerName: !Sub "${Prefix}-GCP_Billing_Crawler"
          PricingCrawlerName: !Sub "${Prefix}-GCP_Pricing_Crawler"
          FocusCrawlerName: !Sub "${Prefix}-GCP_Focus_Crawler"
          GlueJobTrigger: !Sub "${Prefix}-Glue_Jobs_Trigger"
          FOCUSGlueJobTrigger: !Sub "${Prefix}-Glue_FOCUS_Jobs_Trigger"
          GlueCrawlerTrigger: !Sub "${Prefix}-Glue_Crawlers_Trigger"
          FocusGlueCrawlerTrigger: !Sub "${Prefix}-Focus_Glue_Crawler_Trigger"
          GlueCrontab: !Ref GlueCrontab
          GlueWorkflow: !Sub ${Prefix}-workflow
          GlueFocusWorkflow: !Sub ${Prefix}-focus-workflow
          KmsKeyArn: !GetAtt KMSKey.Arn

  GlueRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${Prefix}-job-role
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 
                - s3:ListBucket
                - s3:GetObject
                - s3:PutObject
                - s3:DeleteObject
                - s3:GetBucketAcl
                - s3:GetBucketLocation
                Resource:
                - !Sub "arn:aws:s3:::${Prefix}-${BucketName}"
                - !Sub "arn:aws:s3:::${Prefix}-${BucketName}/*"
              - Effect: Allow
                Action: 
                - s3:ListBucket
                - s3:GetObject
                - s3:PutObject
                - s3:DeleteObject
                - s3:GetBucketAcl
                - s3:GetBucketLocation
                Resource:
                - !Sub "arn:aws:s3:::${Prefix}-${BucketName}-script"
                - !Sub "arn:aws:s3:::${Prefix}-${BucketName}-script/*"
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                Resource: !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${Prefix}-${TargetCatalogDBName}-AdminJobTable"
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:AssociateKmsKey
                Resource: '*'
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                Resource: '*'
              - Effect: Allow
                Action: 
                  - "secretsmanager:GetSecretValue"
                Resource: !Sub "${SecretARN}" 
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:CreateTable
                  - glue:GetSecurityConfigurations
                  - glue:GetSecurityConfiguration
                  - glue:UpdateTable
                  - glue:UpdateDatabase
                  - glue:BatchGetPartition
                  - glue:BatchCreatePartition
                  - glue:BatchUpdatePartition
                  - glue:GetConnection
                Resource: '*'

  GCPLambdaRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Service: glue.amazonaws.com
              Action: sts:AssumeRole
            - Effect: Allow
              Principal:
                Service: lambda.amazonaws.com
              Action: sts:AssumeRole
        Policies:
          - PolicyName: !Sub "${Prefix}-GCPDashboardPolicy"
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action: 
                  - s3:ListBucket
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:GetBucketAcl
                  - s3:GetBucketLocation
                  Resource: !Sub "arn:aws:s3:::${Prefix}-${BucketName}/*"
                - Effect: Allow
                  Action: 
                  - s3:ListBucket
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:GetBucketAcl
                  - s3:GetBucketLocation
                  Resource: !Sub "arn:aws:s3:::${Prefix}-${BucketName}-script/*"
                - Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                    - logs:CreateLogStream
                    - logs:PutLogEvents
                  Resource: '*'
                - Effect: Allow
                  Action:
                    - cloudwatch:PutMetricData
                  Resource: '*'
                - Effect: Allow
                  Action: 
                    - glue:GetDatabase
                    - glue:GetTable
                    - glue:GetJob
                    - glue:CreateJob
                    - glue:GetSecurityConfigurations
                    - glue:GetSecurityConfiguration
                    - glue:UpdateJob
                    - glue:DeleteJob
                    - glue:StartJob
                    - glue:StartJobRun
                    - glue:CreateTable
                    - glue:UpdateTable
                    - glue:UpdateDatabase
                    - glue:BatchGetPartition
                    - glue:BatchCreatePartition
                    - glue:BatchUpdatePartition
                    - glue:CreateTrigger
                    - glue:UpdateTrigger
                    - glue:DeleteTrigger
                    - glue:StartTrigger
                    - glue:GetTrigger
                    - glue:GetTriggers
                    - glue:StartWorkflowRun
                    - glue:ListWorkflows
                    - glue:GetWorkflow
                    - glue:GetWorkflowRun
                    - glue:GetWorkflowRunProperties
                    - glue:GetWorkflowRuns
                  Resource: '*'
                - Effect: Allow
                  Action:
                    - "iam:PassRole"
                  Resource: !GetAtt GlueRole.Arn

  GCPLambdaInvoker:
    Type: Custom::LambdaInvoker
    Properties:
      ServiceToken: !GetAtt GCPLambdaFunction.Arn
      GCPFullTableName: !Join [";", !Ref GCPFullTableName]
      GCPFocusFullTableName: !Join [";", !Ref GCPFocusFullTableName]
      ScriptBucketName: !Sub "${Prefix}-${BucketName}-script"
      ScriptLocation: !Sub "s3://${Prefix}-${BucketName}-script/glue_script.py"      
      FocusScriptLocation: !Sub "s3://${Prefix}-${BucketName}-script/focus_glue_script.py"   
      GCPConnectionName: !Ref GCPConnectionName
      GCPJobBookmarkKeys: !Ref GCPJobBookmarkKeys
      BillingTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPBillingLocation}/"
      PricingTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPPricingLocation}/"
      FocusTargetS3Path: !Sub "s3://${Prefix}-${BucketName}/${GCPFocusLocation}/"
      TargetCatalogDBName: !Ref TargetCatalogDBName
      DynamoDBAdminJobTable: !Select [1, !Split ['/', !GetAtt DynamoDBAdminJobTable.Arn]]
      GCPPricingLocation: !Ref GCPPricingLocation
      GCPPricingFullTableName: !Ref GCPPricingFullTableName
      GlueRoleArn: !GetAtt GlueRole.Arn
      SecurityConfiguration: !Ref GlueSecurityConfiguration
      BillingCrawlerName: !Sub "${Prefix}-GCP_Billing_Crawler"
      PricingCrawlerName: !Sub "${Prefix}-GCP_Pricing_Crawler"
      FocusCrawlerName: !Sub "${Prefix}-GCP_Focus_Crawler"
      GlueJobTrigger: !Sub "${Prefix}-Glue_Jobs_Trigger"
      FOCUSGlueJobTrigger: !Sub "${Prefix}-Glue_FOCUS_Jobs_Trigger"
      GlueCrawlerTrigger: !Sub "${Prefix}-Glue_Crawlers_Trigger"
      GlueCrontab: !Ref GlueCrontab
      GlueWorkflow: !Sub ${Prefix}-workflow
      GlueFocusWorkflow: !Sub ${Prefix}-focus-workflow
      KmsKeyArn: !GetAtt KMSKey.Arn
      

  GlueWorkflow:
    Type: AWS::Glue::Workflow
    Properties:
      Description: "Glue Workflow to manage CID GCP Glue Jobs and Crawlers"
      MaxConcurrentRuns: 100
      Name: !Sub ${Prefix}-workflow

  FocusGlueWorkflow:
    Type: AWS::Glue::Workflow
    Properties:
      Description: "Glue Workflow to manage CID GCP FOCUS Glue Jobs and Crawlers"
      MaxConcurrentRuns: 100
      Name: !Sub ${Prefix}-focus-workflow

  KMSKey:
    Type: "AWS::KMS::Key"
    Properties:
      Description: "GCP CID Dashboard KMS Key"
      #EnableKeyRotation: True # Enable this for strict regulatory requirements
      KeyPolicy:
        Version: '2012-10-17'
        Id: default
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Ref KMSOwners
            Action:
              - kms:Create*
              - kms:Describe*
              - kms:Enable*
              - kms:List*
              - kms:Put*
              - kms:Update*
              - kms:Revoke*
              - kms:Disable*
              - kms:Get*
              - kms:Delete*
              - kms:ScheduleKeyDeletion
              - kms:CancelKeyDeletion
            Resource: "*"
          - Effect: Allow
            Principal:
              AWS: !Ref KMSOwners
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:DescribeKey
            Resource: "*"
          - Effect: Allow
            Principal:
              AWS: 
                - !GetAtt GlueRole.Arn
                - !GetAtt GcpBillingCrawlerRole.Arn
                - !GetAtt LambdaExecutionRole.Arn
                - !GetAtt PutItemLambdaRole.Arn
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:DescribeKey
            Resource: "*"
          - Effect: Allow
            Principal:
              Service:
                - !Sub "logs.${AWS::Region}.amazonaws.com"
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:DescribeKey
            Resource: "*"
      KeyUsage: "ENCRYPT_DECRYPT"
  
  GcpKeyAlias:
      Type: AWS::KMS::Alias
      Properties:
        AliasName: !Sub "alias/${Prefix}-gcpkey"
        TargetKeyId:
          Ref: KMSKey

  GlueSecurityConfiguration:
      Type: AWS::Glue::SecurityConfiguration
      Properties:
        Name: !Sub "${Prefix}-GCPGlueSecurityConfiguration"
        EncryptionConfiguration:
          CloudWatchEncryption: 
            CloudWatchEncryptionMode: SSE-KMS
            KmsKeyArn: !GetAtt KMSKey.Arn
          JobBookmarksEncryption: 
            JobBookmarksEncryptionMode: CSE-KMS 
            KmsKeyArn: !GetAtt KMSKey.Arn
          S3Encryptions: 
            - S3EncryptionMode: SSE-KMS 
              KmsKeyArn: !GetAtt KMSKey.Arn

  GlueCrawler:
    Type: "AWS::Glue::Crawler"
    Properties:
      Role: !GetAtt GcpBillingCrawlerRole.Arn
      CrawlerSecurityConfiguration: !Ref GlueSecurityConfiguration
      DatabaseName: !Ref TargetCatalogDBName
      Name: !Sub "${Prefix}-GCP_Billing_Crawler"
      Description: "Crawler to recursively crawl data from S3 for GCP Billing"
      Targets:
        S3Targets:
          - Path: !Sub "s3://${Prefix}-${BucketName}/${GCPBillingLocation}/" 
            Exclusions: []
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
  
  GluePricingCrawler:
    Type: "AWS::Glue::Crawler"
    Properties:
      Role: !GetAtt GcpBillingCrawlerRole.Arn
      CrawlerSecurityConfiguration: !Ref GlueSecurityConfiguration
      DatabaseName: !Ref TargetCatalogDBName
      Name: !Sub "${Prefix}-GCP_Pricing_Crawler"
      Description: "Crawler to recursively crawl data from S3 for GCP Pricing"
      Targets:
        S3Targets:
          - Path: !Sub "s3://${Prefix}-${BucketName}/${GCPPricingLocation}/" 
            Exclusions: []
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"

  GlueFocusCrawler:
    Type: "AWS::Glue::Crawler"
    Properties:
      Role: !GetAtt GcpBillingCrawlerRole.Arn
      CrawlerSecurityConfiguration: !Ref GlueSecurityConfiguration
      DatabaseName: !Ref TargetCatalogDBName
      Name: !Sub "${Prefix}-GCP_Focus_Crawler"
      Description: "Crawler to recursively crawl data from S3 for GCP FOCUS Format"
      Targets:
        S3Targets:
          - Path: !Sub "s3://${Prefix}-${BucketName}/${GCPFocusLocation}/" 
            Exclusions: []
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE" 

  DynamoDBAdminJobTable:
    Type: AWS::DynamoDB::Table
    Properties:
      SSESpecification:
        SSEEnabled: true
        KMSMasterKeyId: !GetAtt KMSKey.Arn
        SSEType: KMS
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: True
      TableName: !Sub "${Prefix}-${TargetCatalogDBName}-AdminJobTable"
      AttributeDefinitions:
        - AttributeName: "job_id"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "job_id"
          KeyType: "HASH"
      BillingMode: PAY_PER_REQUEST

  PutItemLambdaRole:
    Type: 'AWS::IAM::Role'
    Condition: StartDateNotEmpty
    Properties:
      RoleName: !Sub "${Prefix}-PutItemLambdaRole"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: 'lambda.amazonaws.com'
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: !Sub "${Prefix}-DynamoDBAccessPolicy"
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'dynamodb:PutItem'
                  - 'dynamodb:GetItem'
                  - 'dynamodb:UpdateItem'
                Resource:
                  - !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${Prefix}-${TargetCatalogDBName}-AdminJobTable"
        - PolicyName: !Sub "${Prefix}-PutItemLambdaExecutionPolicy"
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogStream'
                  - 'logs:CreateLogGroup'
                  - 'logs:PutLogEvents'
                  - 'kms:Decrypt'
                Resource:
                  - "*"

  PutItemLambdaFunction:
    Type: 'AWS::Lambda::Function'
    Condition: StartDateNotEmpty
    Properties:
      Handler: 'index.lambda_handler'
      Role: !GetAtt PutItemLambdaRole.Arn
      FunctionName: !Sub "${Prefix}-PutItemLambdaFunction"
      Runtime: 'python3.12'
      Environment:
        Variables:
          TABLE_NAME: !Ref DynamoDBAdminJobTable
          SCRIPT_LOCATION: !Sub "s3://${Prefix}-${BucketName}-script/glue_script.py"
          EXPORT_COLUMN_NAME: 'job_bookmark_column'
          EXPORT_VALUE: !Ref GCPJobBookmarkKeys
          VALUE_COLUMN_NAME: 'job_bookmark_value'
          VALUE: !Ref StartDate
          BILLING_VALUE: !Join [",", !Ref GCPFullTableName]
          PRICING_VALUE: !Ref GCPPricingFullTableName
      Code:
        ZipFile: |
          import os
          import boto3
          import cfnresponse

          table_name = os.environ.get('TABLE_NAME')
          export_column_name = os.environ.get('EXPORT_COLUMN_NAME')
          export_value = os.environ.get('EXPORT_VALUE')
          value_column_name = os.environ.get('VALUE_COLUMN_NAME')
          start_date = os.environ.get('VALUE')
          billing_value = os.environ.get('BILLING_VALUE').split(",")
          pricing_value = os.environ.get('PRICING_VALUE')
          

          def put_item_in_dynamodb(table, column_name, value):
              job = f'GcpApp_{value}'
              try:
                  response = table.put_item(
                      Item={
                          'job_id': job,
                          export_column_name: export_value,
                          value_column_name: start_date
                      }
                  )
                  print("PutItem succeeded:", response)
                  return True, 'Item added successfully'
              except Exception as e:
                  print("Error putting item:", str(e))
                  return False, 'Error adding item to DynamoDB'

          def lambda_handler(event, context):
              dynamodb = boto3.resource('dynamodb')
              error_messages=[]
              success=True

              table = dynamodb.Table(table_name) if table_name else None

              for value in billing_value:
                current_success, current_message = put_item_in_dynamodb(table, export_column_name, value)
                if not current_success:
                    success = False
                    error_messages.append(current_message)
              
              current_success, current_message = put_item_in_dynamodb(table, export_column_name, pricing_value)
              if not current_success:
                  success = False
                  error_messages.append(current_message)

              if success:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, 'Items added successfully')
              else:
                  error_message = ', '.join(error_messages)
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, error_message)
      Timeout: 60

  PutItemIfDate:
    Condition: StartDateNotEmpty
    Type: Custom::PutItemLambdaFunction
    Properties:
      ServiceToken: !GetAtt PutItemLambdaFunction.Arn

  GcpBillingCrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub "${Prefix}-GlueCrawlerPolicy"
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource: 
                  - !Sub "arn:aws:s3:::${Prefix}-${BucketName}"
                  - !Sub "arn:aws:s3:::${Prefix}-${BucketName}/*"
              - Effect: Allow
                Action:
                  - kms:GenerateDataKey
                  - kms:Encrypt
                  - kms:Decrypt
                Resource: 
                  - !Sub "arn:aws:s3:::${Prefix}-${BucketName}"
                  - !Sub "arn:aws:s3:::${Prefix}-${BucketName}/*"
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:AssociateKmsKey
                Resource: '*'
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:CreateTable
                  - glue:GetSecurityConfigurations
                  - glue:GetSecurityConfiguration
                  - glue:UpdateTable
                  - glue:UpdateDatabase
                  - glue:BatchGetPartition
                  - glue:BatchCreatePartition
                  - glue:BatchUpdatePartition
                  - glue:UpdatePartition
                  - glue:GetTriggers
                Resource: "*"

  TargetBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${Prefix}-${BucketName}"
      LoggingConfiguration:
        DestinationBucketName: !Sub "${Prefix}-${BucketName}"
        LogFilePrefix: "access-log"
      VersioningConfiguration:
        Status: "Enabled"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True
      BucketEncryption:
          ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              KMSMasterKeyID: !Sub 'arn:aws:kms:${AWS::Region}:${AWS::AccountId}:alias/${Prefix}-gcpkey'
              SSEAlgorithm: 'aws:kms'
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
  
  TargetBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref TargetBucket
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: AllowSSLOnly
            Action: s3:*
            Effect: Deny
            Principal: "*"
            Resource: !Join ["", [!GetAtt TargetBucket.Arn, "/*"]]
            Condition:
              Bool:
                aws:SecureTransport: false
          - Sid: AllowTLS12Only
            Action: s3:*
            Effect: Deny
            Principal: "*"
            Resource: !Join ["", [!GetAtt TargetBucket.Arn, "/*"]]
            Condition:
              NumericLessThan:
                s3:TlsVersion: 1.2

  ScriptBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${Prefix}-${BucketName}-script"
      LoggingConfiguration:
        DestinationBucketName: !Sub "${Prefix}-${BucketName}-script"
        LogFilePrefix: "access-log"
      VersioningConfiguration:
        Status: "Enabled"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
  
  ScriptBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ScriptBucket
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: AllowSSLOnly
            Action: s3:*
            Effect: Deny
            Principal: "*"
            Resource: !Join ["", [!GetAtt ScriptBucket.Arn, "/*"]]
            Condition:
              Bool:
                aws:SecureTransport: false
          - Sid: AllowTLS12Only
            Action: s3:*
            Effect: Deny
            Principal: "*"
            Resource: !Join ["", [!GetAtt ScriptBucket.Arn, "/*"]]
            Condition:
              NumericLessThan:
                s3:TlsVersion: 1.2

  GlueConnection:
    Type: AWS::Glue::Connection
    Properties:
      CatalogId: !Ref AWS::AccountId
      ConnectionInput:
        Name: !Ref GCPConnectionName
        ConnectionType: "BIGQUERY"
        ConnectionProperties:
          SparkProperties: !Sub '{"secretId": "${Secret}"}'
  
  UploadS3ObjectResourceLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Custom resource for uploading a text content to S3 Bucket
      Code:
        ZipFile: |
          import boto3
          import cfnresponse

          def lambda_handler(event, context):
              request_type = event.get('RequestType', 'Undef')
              region = context.invoked_function_arn.split(":")[3]
              log_url = f"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logEvent:group={context.log_group_name};stream={context.log_stream_name}"
              s3 = boto3.client('s3')
              try:
                  bucket_name = event['ResourceProperties']['Bucket']
                  key = event['ResourceProperties']['Key']
                  content = event['ResourceProperties']['Content']
                  if request_type in ('Create', 'Update') :
                      response = s3.put_object(Bucket=bucket_name, Key=key, Body=content)
                  elif request_type == 'Delete':
                      response = s3.delete_object(Bucket=bucket_name, Key=key) # s3 returns ok if object does not exist https://github.com/boto/boto3/issues/1735
                  else:
                      raise Exception(f'Unexpected request_type = {request_type}')
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response)
              except Exception as exc:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(exc) + ' See logs: ' + log_url})
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12
      ReservedConcurrentExecutions: 100
      Timeout: 60

  UploadScriptOnDeployment:
    Type: Custom::UploadS3ObjectResource
    Properties:
      ServiceToken: !GetAtt UploadS3ObjectResourceLambdaFunction.Arn
      Bucket: !Sub ${Prefix}-${BucketName}-script
      Key: 'glue_script.py'
      Content: |
          import sys
          import os
          from awsglue.transforms import *
          from awsglue.utils import getResolvedOptions
          from pyspark.context import SparkContext
          from awsglue.context import GlueContext
          from awsglue.job import Job
          from pyspark.sql.functions import *
          import boto3
          from botocore.exceptions import ClientError
          import datetime as dt

          args = getResolvedOptions(sys.argv, ['JOB_NAME','gcp_full_table_name', 'gcp_parent_project', 'gcp_materialization_dataset', 'gcp_connection_name','gcp_job_bookmark_keys', 'target_s3_path','target_catalog_database_name', 'target_catalog_table_name','dynamodb_admin_job_table_name'])

          dynamodb_client = boto3.client('dynamodb')

          def source_is_empty(glueContext, dynamic_frame):
              if len(dynamic_frame.toDF().head(1)) == 0:
                  print("No record Extracted! Committing and exiting job without processing.")
                  job.commit()
                  os._exit(0)

          # function to get a column in dataframe       
          def addTechCol(rec):
            rec["partitiondate"] = rec['_PARTITIONDATE']
            rec["partitiontime"] = rec['_PARTITIONTIME']
            del rec["_PARTITIONDATE"]
            del rec["_PARTITIONTIME"]
            return rec


          def getJobBookmarkFromDynamoDb(dynamodb_client,dynamodb_bookmark_table_name,glue_job_name):
              response = dynamodb_client.get_item(TableName=dynamodb_bookmark_table_name, Key={'job_id':{'S':str(glue_job_name)}})
              if 'Item' in response:
                  job_bookmark = response['Item']['job_bookmark_value']['S']
                  return job_bookmark
              else:
                  return '1900-01-01 00:00:00.001'

          def putJobBookmarkToDynamoDb(dynamodb_client,dynamodb_bookmark_table_name,glue_job_name,arg_gcp_job_bookmark_keys,bookmark_value):
              dyn_event = {}
              dyn_event["job_id"] = {'S':str(glue_job_name)}
              dyn_event["job_bookmark_column"] = {'S':str(arg_gcp_job_bookmark_keys)}
              dyn_event["job_bookmark_value"] = {'S':str(bookmark_value)}
              dyn_event["timestamp"] = {'S':str(dt.datetime.utcnow().timestamp()*1000)}
              try:
                    ingest_metadata = dynamodb_client.put_item(TableName=dynamodb_bookmark_table_name, Item=dyn_event)
              except ClientError:
                    msg = 'Error putting item {} into {} table'.format(dyn_event, table)
                    print(msg)
                    raise

          sc = SparkContext()
          glueContext = GlueContext(sc)
          spark = glueContext.spark_session
          job = Job(glueContext)
          job.init(args['JOB_NAME'], args)

          arg_job_name = args['JOB_NAME']
          arg_gcp_parent_project = args['gcp_parent_project']
          arg_gcp_full_table_name = args['gcp_full_table_name']
          arg_gcp_connection_name = args['gcp_connection_name']
          arg_gcp_job_bookmark_keys = args['gcp_job_bookmark_keys']
          arg_gcp_materialization_dataset = args['gcp_materialization_dataset']

          arg_target_s3_path = args['target_s3_path'] 
          arg_target_catalog_database_name = args['target_catalog_database_name']
          arg_target_catalog_table_name = args['target_catalog_table_name']

          arg_dynamodb_bookmark_table_name = args['dynamodb_admin_job_table_name']

          job_bookmark_value=getJobBookmarkFromDynamoDb(dynamodb_client,arg_dynamodb_bookmark_table_name,arg_job_name)
          print(job_bookmark_value)
          # Script generated for node Google BigQuery Connector 0.24.2 for AWS Glue 3.0
          GoogleBigQueryConnector0242forAWSGlue30_node1 = glueContext.create_dynamic_frame.from_options(
              connection_type="bigquery",
              connection_options={
                  #"table": arg_gcp_full_table_name, # Not required asused the query parameter
                  "parentProject": arg_gcp_parent_project,
                  "viewsEnabled": "true",
                  "materializationDataset": f"{arg_gcp_materialization_dataset}",
                  "connectionName": arg_gcp_connection_name,
                  "sourceType": "query",
                  "query": f'SELECT *, EXTRACT(DAY FROM export_time) AS export_day, EXTRACT(MONTH FROM export_time) AS export_month, EXTRACT(YEAR FROM export_time) AS export_year FROM `{arg_gcp_full_table_name}` where {arg_gcp_job_bookmark_keys} >"{job_bookmark_value}"',
              },
              transformation_ctx="GoogleBigQueryConnector0242forAWSGlue30_node1",
          )

          #Check if the source dataset is empty. If empty exit.
          source_is_empty(glueContext,GoogleBigQueryConnector0242forAWSGlue30_node1)

          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.coalesce(1)

          print(GoogleBigQueryConnector0242forAWSGlue30_node1.schema())
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("_partitiondate", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("_partitiontime", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("usage_end_time", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("usage_start_time", "cast:timestamp")])
          #GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.map(f = addTechCol)
          GoogleBigQueryConnector0242forAWSGlue30_node1.printSchema()


          # Script generated for node Amazon S3
          AmazonS3_node = glueContext.getSink(
              path=arg_target_s3_path,
              connection_type="s3",
              updateBehavior="UPDATE_IN_DATABASE",
              partitionKeys=["billing_account_id","export_year","export_month","export_day"],
              compression="snappy",
              enableUpdateCatalog=True,
              transformation_ctx="AmazonS3_node",
          )
          
          AmazonS3_node.setCatalogInfo(
              catalogDatabase=arg_target_catalog_database_name, catalogTableName=arg_target_catalog_table_name
          )
          AmazonS3_node.setFormat("glueparquet")
          AmazonS3_node.writeFrame(GoogleBigQueryConnector0242forAWSGlue30_node1)

          max_bookmark_column = GoogleBigQueryConnector0242forAWSGlue30_node1.toDF().select(max(arg_gcp_job_bookmark_keys)).collect()[0][0]
          max_bookmark_column = max_bookmark_column.strftime("%Y-%m-%d %H:%M:%S.%f")
          print(max_bookmark_column)
          putJobBookmarkToDynamoDb(dynamodb_client,arg_dynamodb_bookmark_table_name,arg_job_name,arg_gcp_job_bookmark_keys,max_bookmark_column)
          job.commit()
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12

  FocusUploadScriptOnDeployment:
    Type: Custom::UploadS3ObjectResource
    Properties:
      ServiceToken: !GetAtt UploadS3ObjectResourceLambdaFunction.Arn
      Bucket: !Sub ${Prefix}-${BucketName}-script
      Key: 'focus_glue_script.py'
      Content: |
          import sys
          import os
          from awsglue.transforms import *
          from awsglue.utils import getResolvedOptions
          from pyspark.context import SparkContext
          from awsglue.context import GlueContext
          from awsglue.job import Job
          from pyspark.sql.functions import *
          import boto3
          from botocore.exceptions import ClientError
          import datetime as dt

          args = getResolvedOptions(sys.argv, ['JOB_NAME','gcp_full_table_name', 'gcp_parent_project', 'gcp_materialization_dataset', 'gcp_connection_name','gcp_job_bookmark_keys', 'target_s3_path','target_catalog_database_name', 'target_catalog_table_name','dynamodb_admin_job_table_name'])

          dynamodb_client = boto3.client('dynamodb')

          def source_is_empty(glueContext, dynamic_frame):
              if len(dynamic_frame.toDF().head(1)) == 0:
                  print("No record Extracted! Committing and exiting job without processing.")
                  job.commit()
                  os._exit(0)

          # function to get a column in dataframe       
          def addTechCol(rec):
            rec["partitiondate"] = rec['_PARTITIONDATE']
            rec["partitiontime"] = rec['_PARTITIONTIME']
            del rec["_PARTITIONDATE"]
            del rec["_PARTITIONTIME"]
            return rec


          def getJobBookmarkFromDynamoDb(dynamodb_client,dynamodb_bookmark_table_name,glue_job_name):
              response = dynamodb_client.get_item(TableName=dynamodb_bookmark_table_name, Key={'job_id':{'S':str(glue_job_name)}})
              if 'Item' in response:
                  job_bookmark = response['Item']['job_bookmark_value']['S']
                  return job_bookmark
              else:
                  return '1900-01-01 00:00:00.001'

          def putJobBookmarkToDynamoDb(dynamodb_client,dynamodb_bookmark_table_name,glue_job_name,arg_gcp_job_bookmark_keys,bookmark_value):
              dyn_event = {}
              dyn_event["job_id"] = {'S':str(glue_job_name)}
              dyn_event["job_bookmark_column"] = {'S':str(arg_gcp_job_bookmark_keys)}
              dyn_event["job_bookmark_value"] = {'S':str(bookmark_value)}
              dyn_event["timestamp"] = {'S':str(dt.datetime.utcnow().timestamp()*1000)}
              try:
                    ingest_metadata = dynamodb_client.put_item(TableName=dynamodb_bookmark_table_name, Item=dyn_event)
              except ClientError:
                    msg = 'Error putting item {} into {} table'.format(dyn_event, table)
                    print(msg)
                    raise

          sc = SparkContext()
          glueContext = GlueContext(sc)
          spark = glueContext.spark_session
          job = Job(glueContext)
          job.init(args['JOB_NAME'], args)

          arg_job_name = args['JOB_NAME']
          arg_gcp_parent_project = args['gcp_parent_project']
          arg_gcp_full_table_name = args['gcp_full_table_name']
          arg_gcp_connection_name = args['gcp_connection_name']
          arg_gcp_job_bookmark_keys = args['gcp_job_bookmark_keys']
          arg_gcp_materialization_dataset = args['gcp_materialization_dataset']

          arg_target_s3_path = args['target_s3_path'] 
          arg_target_catalog_database_name = args['target_catalog_database_name']
          arg_target_catalog_table_name = args['target_catalog_table_name']

          arg_dynamodb_bookmark_table_name = args['dynamodb_admin_job_table_name']

          job_bookmark_value=getJobBookmarkFromDynamoDb(dynamodb_client,arg_dynamodb_bookmark_table_name,arg_job_name)
          print(job_bookmark_value)
          # Script generated for node Google BigQuery Connector 0.24.2 for AWS Glue 3.0
          GoogleBigQueryConnector0242forAWSGlue30_node1 = glueContext.create_dynamic_frame.from_options(
              connection_type="bigquery",
              connection_options={
                  #"table": arg_gcp_full_table_name, # Not required asused the query parameter
                  "parentProject": arg_gcp_parent_project,
                  "viewsEnabled": "true",
                  "materializationDataset": f"{arg_gcp_materialization_dataset}",
                  "connectionName": arg_gcp_connection_name,
                  "sourceType": "query",
                  "query": f'SELECT *, EXTRACT(DAY FROM x_ExportTime) AS export_day, EXTRACT(MONTH FROM x_ExportTime) AS export_month, EXTRACT(YEAR FROM x_ExportTime) AS export_year FROM `{arg_gcp_full_table_name}` where {arg_gcp_job_bookmark_keys} >"{job_bookmark_value}"',
              },
              transformation_ctx="GoogleBigQueryConnector0242forAWSGlue30_node1",
          )

          #Check if the source dataset is empty. If empty exit.
          source_is_empty(glueContext,GoogleBigQueryConnector0242forAWSGlue30_node1)

          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.coalesce(1)

          print(GoogleBigQueryConnector0242forAWSGlue30_node1.schema())
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("_partitiondate", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("_partitiontime", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("usage_end_time", "cast:timestamp")])
          GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.resolveChoice(specs=[("usage_start_time", "cast:timestamp")])
          #GoogleBigQueryConnector0242forAWSGlue30_node1 = GoogleBigQueryConnector0242forAWSGlue30_node1.map(f = addTechCol)
          GoogleBigQueryConnector0242forAWSGlue30_node1.printSchema()


          # Script generated for node Amazon S3
          AmazonS3_node = glueContext.getSink(
              path=arg_target_s3_path,
              connection_type="s3",
              updateBehavior="UPDATE_IN_DATABASE",
              partitionKeys=["BillingAccountId","export_year","export_month","export_day"],
              compression="snappy",
              enableUpdateCatalog=True,
              transformation_ctx="AmazonS3_node",
          )

          AmazonS3_node.setCatalogInfo(
              catalogDatabase=arg_target_catalog_database_name, catalogTableName=arg_target_catalog_table_name
          )
          AmazonS3_node.setFormat("glueparquet")
          AmazonS3_node.writeFrame(GoogleBigQueryConnector0242forAWSGlue30_node1)

          max_bookmark_column = GoogleBigQueryConnector0242forAWSGlue30_node1.toDF().select(max(arg_gcp_job_bookmark_keys)).collect()[0][0]
          max_bookmark_column = max_bookmark_column.strftime("%Y-%m-%d %H:%M:%S.%f")
          print(max_bookmark_column)
          putJobBookmarkToDynamoDb(dynamodb_client,arg_dynamodb_bookmark_table_name,arg_job_name,arg_gcp_job_bookmark_keys,max_bookmark_column)
          job.commit()
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12
  
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub "${Prefix}-LambdaS3Access"
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource: !Sub "arn:aws:s3:::${Prefix}-${BucketName}-script/*"
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
  
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub ${TargetCatalogDBName}
        Description: "GCP Billing and Pricing Glue Database"
